{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Korean_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTfPzcNnlN0eiC8EPE9Tze"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"zsFOY6a6Tpr7"},"source":["# corpus 얻기 위한 부분 -> 선택적인 코드\n","!pip install newspaper3k\n","import newspaper\n","newspaper.languages()\n","news_url = \"https://kimziont.github.io/nlp_ustage/week3_special_lecture_1/\"\n","article = newspaper.Article(news_url, language='ko')\n","article.download()\n","article.parse()\n","context = article.text.split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVeieNl4E2qM","executionInfo":{"status":"ok","timestamp":1632747197304,"user_tz":-540,"elapsed":347,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# 문장 단위 tokenization\n","def sentence_tokenization(texts):\n","    flag = True\n","    while flag:\n","        try:\n","            import kss\n","            flag = False\n","        except ModuleNotFoundError:\n","            answer = input(\"You need 'kss' module do you want to install it? [yes/no]\")\n","            if answer == \"yes\":\n","                ! pip install kss\n","            elif answer == \"no\":\n","                print(\"'sentence_tokenization' is terminated\")\n","                return texts\n","            else:\n","                answer = input(\"Wrong answer, Do you want to install 'kss' [yes/no]\")\n","\n","    sents = []\n","    for sent in texts:\n","        sent = sent.strip()\n","        if sent:\n","            splited_sent = kss.split_sentences(sent)\n","            sents.extend(splited_sent)\n","    return sents"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FqvZiRtMCGK","executionInfo":{"status":"ok","timestamp":1632747360365,"user_tz":-540,"elapsed":577,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# html 태그 제거\n","def remove_html(texts):\n","    import re\n","    \"\"\"\n","    HTML 태그를 제거합니다.\n","    ``<p>안녕하세요 ㅎㅎ </p>`` -> ``안녕하세요 ㅎㅎ ``\n","    \"\"\"\n","    \n","    preprcessed_text = []\n","    for text in texts:\n","        text = re.sub(r\"<[^>]+>\\s+(?=<)|<[^>]+>\", \"\", text).strip()\n","        if text:\n","            preprcessed_text.append(text)\n","    return preprcessed_text"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8Qp_YKKFYeq","executionInfo":{"status":"ok","timestamp":1632747360827,"user_tz":-540,"elapsed":1,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# 두 개 이상의 연속된 공백을 하나로 치환\n","def remove_repeated_spacing(corpus):\n","    import re\n","    \"\"\"\n","    두 개 이상의 연속된 공백을 하나로 치환합니다.\n","    ``오늘은    날씨가   좋다.`` -> ``오늘은 날씨가 좋다.``\n","    \"\"\"\n","\n","    preprocessed_text = []\n","    for text in corpus:\n","        text = re.sub(r\"\\s+\", \" \", text).strip()\n","        if text:\n","            preprocessed_text.append(text)\n","    return preprocessed_text"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbg850CaIYM3","executionInfo":{"status":"ok","timestamp":1632747361240,"user_tz":-540,"elapsed":3,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["# 반복되는 문자 num_repeats개 만큼만 사용\n","def remove_repeat_char(texts):\n","    flag = True\n","    while flag:\n","        try:\n","            from soynlp.normalizer import repeat_normalize\n","            flag = False\n","        except ModuleNotFoundError:\n","            answer = input(\"You need 'soynlp' module do you want to install it? [yes/no]\")\n","            if answer == \"yes\":\n","                ! pip install soynlp\n","            elif answer == \"no\":\n","                print(\"'remove_repeat_char' is terminated\")\n","                return texts\n","            else:\n","                print(\"Wrong answer\")\n","\n","    preprocessed_text = []\n","    for text in texts:\n","        text = repeat_normalize(text, num_repeats=3).strip()\n","        if text:\n","            preprocessed_text.append(text)\n","    return preprocessed_text"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9JPlKngMagD","executionInfo":{"status":"ok","timestamp":1632749230632,"user_tz":-540,"elapsed":551,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}}},"source":["class PreprocesKorean:\n","    def __init__(self, texts):\n","        self.texts = texts\n","        self.preprocess_dict = {0: \"sentence_tokenization\", 1: \"remove_html\", 2: \"remove_repeated_spacing\",\n","                                3: \"remove_repeat_char\"}\n","        print(\"Choose numbers to preprocessing what you want. you can check through 'preproces_dict' attribute\")\n","    \n","    def __call__(self, *nums):\n","        preprocessed_texts = self.texts\n","        for num in nums:\n","            print(f\"{self.preprocess_dict[num]} is in process!\")\n","            preprocessed_texts = eval(f\"{self.preprocess_dict[num]}(preprocessed_texts)\")\n","        print(\"preprocessing is completed!\\n\")\n","        return preprocessed_texts"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gT7sSBl-QIBe","executionInfo":{"status":"ok","timestamp":1632749235572,"user_tz":-540,"elapsed":4942,"user":{"displayName":"JaeYeong Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00938800364689824387"}},"outputId":"664c1b3f-3b99-4312-e7e7-544c9f910812"},"source":["preprocessing_kor = PreprocesKorean(context)\n","\n","preprocessed_texts = preprocessing_kor(0, 1, 2, 3)\n","\n","for idx, text in enumerate(preprocessed_texts):\n","    print(idx, text)"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Choose numbers to preprocessing what you want. you can check through 'preproces_dict' attribute\n","sentence_tokenization is in process!\n","remove_html is in process!\n","remove_repeated_spacing is in process!\n","remove_repeat_char is in process!\n","preprocessing is completed!\n","\n","0 부스트캠프 NLP 3주차 특강: 서비스향 AI모델 개발하기\n","1 AI를 공부하다보면 실제 현업에서는 어떤 프로세스를 거쳐 인공지능 모델이 개발되고 서비스 까지 나아갈 수 있을까?\n","2 에 대한 고민을 한 적이 있을겁니다.\n","3 이번 포스트는 업스테이지 CTO 이활석님께서 강연하신 서비스향 AI모델 개발하기를 바탕으로 포스트를 정리하였습니다.\n","4 1. 학습 데이터셋 준비\n","5 1) 현업에서는 데이터가 주어지지 않는 경우가 많다\n","6 학교에서 경험한 AI 모델 개발은 보통 주어진 데이터를 preprocessing하고 EDA를 통해 얻은 insight에 근거해 모델을 개발하게 됩니다. 하지만 현업에서는 데이터가 주어지지 않는 경우가 많습니다.\n","7 2) 데이터를 어떤 점에 초점을 맞추고 만들 것인가\n","8 데이터를 만들기 위해서는 우선 서비스 요구사항 을 확인해야 합니다.\n","9 예시는 강의에서 제공된 자료를 통해 살펴보겠습니다.\n","10 위의 그림과 같이 서비스 기획팀과 끊임없는 대화를 통해 어떤 점을 고려하여 서비스를 제공할 지 방향성을 잡아야 이에 기반해 데이터를 구성할 수 있습니다.\n","11 또한 이러한 과정과 모델을 개발하는 과정은 순서가 명확하게 나뉘는 것이 아니라, 반복적인 과정을 통해 계속 상호 보완 해야 합니다.\n","12 3) 외주업체와의 소통\n","13 데이터를 수집, 생성할 때 어떤 점에 초점을 맞출지 정해졌다면, 그리고 이러한 과정을 외주업체에게 위탁하려는 경우 다음 사항을 생각해볼 수 있습니다.\n","14 작업 가이드 작성\n","15 작업 단가 논의\n","16 작업 수량 논의\n","17 QnA 대응\n","18 2. 모델 개발\n","19 모델에 test해볼 수 있는 최소한의 데이터셋이 확보되었다면 그에 맞게 모델을 개발하면 됩니다.\n","20 모델을 개발할 때에도 고려해야할 사항들이 있습니다.\n","21 처리 시간 예) 이미지 입력/촬영 후 수식 정보가 출력될 때 까지의 시간\n","22 목표 정확도 사용자 만족도, 사용자로부터 수정 피드백이 올 확률\n","23 목표 qps(queries per second) 초당 처리 요청 수 (장비를 늘리거나 처리 시간, 모델 크기를 줄이는 방식으로)\n","24 Serving 방식 기술 모듈이 Mobile, Local, Cloud 등 어디서 동작하기 원하는지\n","25 장비 사양\n","26 3. 현업에서의 역할 분담\n","27 보통 인공지능을 공부할 때는 데이터는 이미 주어져 있기 때문에, 대부분의 시간을 모델 개발에 사용하게 됩니다. 하지만 현업에서는 이 밖에도 맡아야 할 일들이 많습니다.\n","28 1) 데이터 준비/분석, 모델 개발, 툴 개발\n","29 2) 모델 최적화\n","30 3) 모델 서빙\n","31 이렇게 인공지능 모델을 개발해 서비스화하기 까지에는 세세한 과정이 있기에 여러 분야들을 조금씩 배우며 자신이 좋아하거나 잘하는 분야에 조금 더 집중하면 좋을 것 같습니다.\n","32 4. 참조\n","33 Upstage CTO 이활석님의 ‘서비스향 AI모델 개발하기’ 특강\n"]}]}]}